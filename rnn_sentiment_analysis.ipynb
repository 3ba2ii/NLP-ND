{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Part 2\n",
    "\n",
    "_Natural Langauge Processing Nanodegree Program_\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Switching gears - RNNs\n",
    "\n",
    "We just saw how the task of sentiment analysis can be solved via a traditional machine learning approach: BoW + a nonlinear classifier. We now switch gears and use Recurrent Neural Networks, and in particular LSTMs, to perform sentiment analysis in Keras. Conveniently, Keras has a built-in [IMDb movie reviews dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) that we can use, with the same vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n",
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb  # import the built-in imdb dataset in Keras\n",
    "\n",
    "# Set the vocabulary size\n",
    "vocabulary_size = 5000\n",
    "\n",
    "# Load in training and test data (note the difference in convention compared to scikit-learn)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print(\"Loaded dataset with {} training samples, {} test samples\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Review ---\n",
      "[1, 4, 698, 1071, 396, 1510, 122, 24, 2, 4402, 19, 4, 326, 7, 2, 4, 3023, 4132, 466, 15, 1063, 5, 2, 2, 12, 16, 2429, 8, 2242, 2, 5, 2, 3286, 2, 5, 1248, 349, 8, 521, 4, 698, 2, 134, 84, 71, 220, 1097, 2, 39, 4, 655, 4132, 54, 4, 1071, 2, 69, 2, 8, 123, 68, 290, 4, 2, 7, 4, 2, 71, 1412, 8, 98, 247, 74, 30, 2, 33, 344, 34, 2, 5, 2, 17, 69, 2453, 77, 4, 420, 36, 2, 8, 2, 4, 2, 2, 513, 38, 2, 5, 2, 877, 572, 1063, 19, 15, 707, 7, 4069, 4, 4311, 2159, 7, 1071, 2, 562, 68, 2, 39, 2358, 180, 4, 1031, 2407, 827, 2115, 382, 4, 91, 804, 1071, 396, 2, 126, 16, 4, 492, 7, 6, 704, 2, 24, 6, 2, 4875, 1346, 11, 522, 4575, 1831, 6, 704, 11, 4, 2, 7, 1208, 2, 3237, 5, 2, 4, 236, 7, 94, 2, 8, 94, 1138, 2, 17, 946, 17, 2, 2613, 100, 2, 125, 27, 2, 2, 2, 2, 48, 16, 2, 19, 2, 1805, 34, 4, 2, 4132, 5, 3419, 2, 34, 316, 334, 12, 215, 30, 2032, 15, 4, 38, 446, 1506, 7, 119, 16, 1477, 34, 4, 2, 2634, 6, 701, 1494, 15, 317, 6, 171, 2, 11, 1316, 19, 2, 1828, 5, 4, 1206, 590, 2, 19, 31, 42, 107, 1912, 2, 21, 3301, 427, 164, 38, 54, 443, 2, 22, 3820, 4, 2, 4, 2, 1138, 2, 2, 2, 11, 51, 36, 219, 17, 443, 2, 2, 4149, 2, 120, 35, 2, 98, 466, 4, 192, 15, 29, 16, 2, 2, 180, 33, 4, 130, 6, 1949, 15, 62, 28, 4064, 2, 98, 69, 36, 4102, 11, 68, 1027, 8, 79, 83, 4, 1206, 590, 2, 4, 22, 579, 178, 164, 162, 44, 1071, 2, 2, 2, 1043, 2, 2, 2, 2, 710, 2, 35, 32, 99, 1081, 1029, 12, 16, 2, 8, 30, 35, 2, 4433, 3513, 8, 4, 3408, 2, 926, 4359, 34, 4, 396, 73, 754, 4212, 153, 23, 4, 172, 2, 26, 131, 2, 4, 698, 1031, 1510, 47, 24, 1194, 4, 2, 2634, 47, 77, 196, 1551, 549, 34, 148, 574, 23, 31, 499, 42, 4, 85, 5, 443, 2548, 47, 2, 27, 1110, 7, 4, 704, 2355, 8, 57, 962, 1689, 48, 34, 101, 580, 25, 144, 657, 8, 332, 6, 274, 44, 2, 37, 122, 2999, 4, 1510, 247, 53, 2976, 74, 443, 2, 5, 27, 369, 5, 4100, 1452, 8, 376, 4, 787, 169, 6, 1039, 7, 2, 1001, 398, 34, 4, 132, 625, 76, 2, 4894, 48, 2, 45, 403, 8, 443, 2, 22, 6, 132, 1447, 11, 2, 2853, 6, 132, 625, 157, 9, 2, 112, 2, 46, 7, 263, 2, 479, 5, 81, 12, 159, 4, 2, 4132, 3115, 11, 2, 90, 6, 701, 415, 382, 1843, 144, 380, 6, 2634, 44, 15]\n",
      "--- Label ---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample review and its label\n",
    "print(\"--- Review ---\")\n",
    "print(X_train[7])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the label is an integer (0 for negative, 1 for positive), and the review itself is stored as a sequence of integers. These are word IDs that have been preassigned to individual words. To map them back to the original words, you can use the dictionary returned by `imdb.get_word_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "--- Review (with words) ---\n",
      "['the', 'of', 'non', 'tension', 'doing', 'wall', 'off', 'his', 'and', 'treats', 'film', 'of', 'less', 'br', 'and', 'of', 'bridge', 'props', 'throughout', 'for', 'members', 'to', 'and', 'and', 'that', 'with', 'qualities', 'in', 'dry', 'and', 'to', 'and', 'stunts', 'and', 'to', 'aspect', 'budget', 'in', 'actress', 'of', 'non', 'and', 'while', 'great', 'than', 'family', 'bored', 'and', 'or', 'of', 'husband', 'props', 'no', 'of', 'tension', 'and', 'me', 'and', 'in', 'ever', 'were', 'main', 'of', 'and', 'br', 'of', 'and', 'than', 'cry', 'in', 'any', 'girl', 'been', 'at', 'and', 'they', 'line', 'who', 'and', 'to', 'and', 'movie', 'me', 'deaths', 'will', 'of', 'liked', 'from', 'and', 'in', 'and', 'of', 'and', 'and', 'kill', 'her', 'and', 'to', 'and', 'killing', 'happened', 'members', 'film', 'for', 'silly', 'br', 'unintentional', 'of', 'hal', 'pair', 'br', 'tension', 'and', 'strong', 'were', 'and', 'or', 'baseball', 'things', 'of', 'remake', 'cutting', 't', 'stereotypes', 'came', 'of', 'its', 'dialog', 'tension', 'doing', 'and', 'your', 'with', 'of', 'works', 'br', 'is', 'class', 'and', 'his', 'is', 'and', 'justin', 'marriage', 'this', 'etc', 'yelling', 'issue', 'is', 'class', 'this', 'of', 'and', 'br', 'tough', 'and', 'terrifying', 'to', 'and', 'of', 'performance', 'br', 'make', 'and', 'in', 'make', 'common', 'and', 'movie', 'towards', 'movie', 'and', 'melodrama', 'after', 'and', 'better', 'be', 'and', 'and', 'and', 'and', 'what', 'with', 'and', 'film', 'and', 'jeff', 'who', 'of', 'and', 'props', 'to', 'dawn', 'and', 'who', 'seeing', 'fan', 'that', \"isn't\", 'at', 'frankly', 'for', 'of', 'her', 'oh', 'porn', 'br', 'did', 'with', 'kelly', 'who', 'of', 'and', 'wear', 'is', 'country', 'share', 'for', 'half', 'is', 'again', 'and', 'this', 'managed', 'film', 'and', 'england', 'to', 'of', 'f', 'happen', 'and', 'film', 'by', \"it's\", 'seen', 'hopes', 'and', 'not', 'widmark', 'boy', 'director', 'her', 'no', 'called', 'and', 'you', 'triumph', 'of', 'and', 'of', 'and', 'common', 'and', 'and', 'and', 'this', 'when', 'from', 'least', 'movie', 'called', 'and', 'and', 'consistently', 'and', 'show', 'so', 'and', 'any', 'throughout', 'of', 'enough', 'for', 'all', 'with', 'and', 'and', 'things', 'they', 'of', 'here', 'is', 'mike', 'for', 'story', 'one', 'resemblance', 'and', 'any', 'me', 'from', 'apes', 'this', 'were', 'manages', 'in', 'also', 'first', 'of', 'f', 'happen', 'and', 'of', 'you', 'coming', 'want', 'director', 'actually', 'has', 'tension', 'and', 'and', 'and', 'mentioned', 'and', 'and', 'and', 'and', \"aren't\", 'and', 'so', 'an', 'movies', 'spent', 'odd', 'that', 'with', 'and', 'in', 'at', 'so', 'and', 'report', 'handful', 'in', 'of', 'revelation', 'and', 'footage', 'wore', 'who', 'of', 'doing', 'much', 'points', 'blake', 'actors', 'are', 'of', 'every', 'and', 'he', 'these', 'and', 'of', 'non', 'remake', 'wall', 'there', 'his', 'decide', 'of', 'and', 'wear', 'there', 'will', 'both', 'flicks', 'type', 'who', 'though', 'daughter', 'are', 'by', 'able', \"it's\", 'of', 'because', 'to', 'called', 'gas', 'there', 'and', 'be', 'trouble', 'br', 'of', 'class', 'lower', 'in', 'even', 'personal', 'machine', 'what', 'who', 'think', 'lack', 'have', 'real', 'saying', 'in', \"you're\", 'is', 'ending', 'has', 'and', 'like', 'off', 'stupidity', 'of', 'wall', 'girl', 'up', 'screenwriter', 'been', 'called', 'and', 'to', 'be', 'women', 'to', 'politically', 'hotel', 'in', 'stupid', 'of', '9', 'same', 'is', 'doctor', 'br', 'and', 'inside', 'keep', 'who', 'of', 'say', 'david', 'get', 'and', 'marty', 'what', 'and', 'if', 'human', 'in', 'called', 'and', 'you', 'is', 'say', 'paris', 'this', 'and', \"could've\", 'is', 'say', 'david', 'another', 'it', 'and', 'never', 'and', 'some', 'br', 'comes', 'and', 'guess', 'to', 'people', 'that', 'new', 'of', 'and', 'props', 'reed', 'this', 'and', 'made', 'is', 'country', 'piece', 'came', 'lovers', 'real', 'sex', 'is', 'wear', 'has', 'for']\n",
      "--- Label ---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Map word IDs back to words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print(\"--- Review (with words) ---\")\n",
    "print([id2word.get(i, \" \") for i in X_train[7]])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1856"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id.get('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our Bag-of-Words approach, where we simply summarized the counts of each word in a document, this representation essentially retains the entire sequence of words (minus punctuation, stopwords, etc.). This is critical for RNNs to function. But it also means that now the features can be of different lengths!\n",
    "\n",
    "#### Question: Variable length reviews\n",
    "\n",
    "What is the maximum review length (in terms of number of words) in the training set? What is the minimum?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "Maximum is guess will be 500 words per review and the minumum will be None i guess\n",
    "\n",
    "### TODO: Pad sequences\n",
    "\n",
    "In order to feed this data into your RNN, all input documents must have the same length. Let's limit the maximum review length to `max_words` by truncating longer reviews and padding shorter reviews with a null value (0). You can accomplish this easily using the [`pad_sequences()`](https://keras.io/preprocessing/sequence/#pad_sequences) function in Keras. For now, set `max_words` to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Set the maximum number of words per document (for both training and testing)\n",
    "max_words = 500\n",
    "\n",
    "# TODO: Pad sequences in X_train and X_test\n",
    "X_train = sequence.pad_sequences(maxlen=max_words,value=0,sequences=X_train)\n",
    "X_test = sequence.pad_sequences(maxlen=max_words,value=0,sequences=X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Design an RNN model for sentiment analysis\n",
    "\n",
    "Build your model architecture in the code cell below. We have imported some layers from Keras that you might need but feel free to use any other layers / transformations you like.\n",
    "\n",
    "Remember that your input is a sequence of words (technically, integer word IDs) of maximum length = `max_words`, and your output is a binary sentiment label (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 500, 64)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 353,089\n",
      "Trainable params: 353,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# TODO: Design your model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size,output_dim=64,input_length=max_words))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Architecture and parameters\n",
    "\n",
    "Briefly describe your neural net architecture. How many model parameters does it have that need to be trained?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "...\n",
    "\n",
    "### TODO: Train and evaluate your model\n",
    "\n",
    "Now you are ready to train your model. In Keras world, you first need to _compile_ your model by specifying the loss function and optimizer you want to use while training, as well as any evaluation metrics you'd like to measure. Specify the approprate parameters, including at least one metric `'accuracy'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 329s 16ms/step - loss: 0.4653 - acc: 0.7829 - val_loss: 0.3209 - val_acc: 0.8642\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 327s 16ms/step - loss: 0.3035 - acc: 0.8801 - val_loss: 0.4941 - val_acc: 0.7742\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 328s 16ms/step - loss: 0.2652 - acc: 0.8969 - val_loss: 0.3087 - val_acc: 0.8734\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 327s 16ms/step - loss: 0.2346 - acc: 0.9108 - val_loss: 0.3350 - val_acc: 0.8752\n",
      "Epoch 5/7\n",
      "20000/20000 [==============================] - 326s 16ms/step - loss: 0.2145 - acc: 0.9189 - val_loss: 0.3165 - val_acc: 0.8650\n",
      "Epoch 6/7\n",
      "20000/20000 [==============================] - 326s 16ms/step - loss: 0.2012 - acc: 0.9250 - val_loss: 0.3126 - val_acc: 0.8804\n",
      "Epoch 7/7\n",
      "20000/20000 [==============================] - 327s 16ms/step - loss: 0.1875 - acc: 0.9327 - val_loss: 0.3180 - val_acc: 0.8714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c1b973198>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compile your model, specifying a loss function, optimizer, and metrics\n",
    "model.fit(X_train, y_train, epochs=7, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once compiled, you can kick off the training process. There are two important training parameters that you have to specify - **batch size** and **number of training epochs**, which together with your model architecture determine the total training time.\n",
    "\n",
    "Training may take a while, so grab a cup of coffee, or better, go for a hike! If possible, consider using a GPU, as a single training run can take several hours on a CPU.\n",
    "\n",
    "> **Tip**: You can split off a small portion of the training set to be used for validation during training. This will help monitor the training process and identify potential overfitting. You can supply a validation set to `model.fit()` using its `validation_data` parameter, or just specify `validation_split` - a fraction of the training data for Keras to set aside for this purpose (typically 5-10%). Validation metrics are evaluated once at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save your model, so that you can quickly load it in future (and perhaps resume training)\n",
    "model_file = \"rnn_model.h5\"  # HDF5 file\n",
    "cache_dir='./'\n",
    "model.save(os.path.join(cache_dir, model_file))\n",
    "\n",
    "# Later you can load it using keras.models.load_model()\n",
    "#from keras.models import load_model\n",
    "#model = load_model(os.path.join(cache_dir, model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have trained your model, it's time to see how well it performs on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.87148\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# RegEx for removing non-letter characters\n",
    "import re\n",
    "\n",
    "# NLTK library for the remaining steps\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")   # download list of stopwords (only once; need not run it again)\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1856"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id.get(\"unknown\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 107s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = model.predict_classes(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Label Prediction\n",
       "0      Negative   Negative\n",
       "1      Negative   Negative\n",
       "2      Negative   Positive\n",
       "3      Positive   Positive\n",
       "4      Negative   Negative\n",
       "5      Negative   Positive\n",
       "6      Positive   Positive\n",
       "7      Negative   Positive\n",
       "8      Negative   Negative\n",
       "9      Positive   Positive\n",
       "10     Positive   Positive\n",
       "11     Positive   Positive\n",
       "12     Negative   Negative\n",
       "13     Positive   Positive\n",
       "14     Negative   Negative\n",
       "15     Positive   Positive\n",
       "16     Positive   Positive\n",
       "17     Positive   Positive\n",
       "18     Negative   Negative\n",
       "19     Positive   Positive\n",
       "20     Positive   Positive\n",
       "21     Negative   Positive\n",
       "22     Negative   Negative\n",
       "23     Negative   Negative\n",
       "24     Positive   Negative\n",
       "25     Positive   Positive\n",
       "26     Positive   Positive\n",
       "27     Negative   Negative\n",
       "28     Negative   Negative\n",
       "29     Negative   Negative\n",
       "...         ...        ...\n",
       "24970  Positive   Positive\n",
       "24971  Negative   Positive\n",
       "24972  Positive   Positive\n",
       "24973  Negative   Negative\n",
       "24974  Positive   Positive\n",
       "24975  Negative   Positive\n",
       "24976  Positive   Positive\n",
       "24977  Positive   Positive\n",
       "24978  Negative   Negative\n",
       "24979  Positive   Positive\n",
       "24980  Positive   Positive\n",
       "24981  Negative   Negative\n",
       "24982  Negative   Negative\n",
       "24983  Positive   Negative\n",
       "24984  Negative   Negative\n",
       "24985  Negative   Positive\n",
       "24986  Negative   Negative\n",
       "24987  Negative   Negative\n",
       "24988  Positive   Positive\n",
       "24989  Positive   Positive\n",
       "24990  Negative   Negative\n",
       "24991  Negative   Negative\n",
       "24992  Negative   Positive\n",
       "24993  Positive   Positive\n",
       "24994  Negative   Negative\n",
       "24995  Negative   Negative\n",
       "24996  Positive   Positive\n",
       "24997  Positive   Positive\n",
       "24998  Negative   Negative\n",
       "24999  Positive   Positive\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "def visualize_prediction(output):\n",
    "    outputs =[]\n",
    "    for value in output:\n",
    "        outputs.append(*value)\n",
    "\n",
    "    output=list(output)\n",
    "    df = pd.DataFrame(data={'Label':list(y_test), 'Prediction':outputs })\n",
    "\n",
    "\n",
    "    df['Label'],df['Prediction']=df.Label.map({0:'Positive',1:'Negative'}),df.Prediction.map({0:'Positive',1:'Negative'})\n",
    "    return df\n",
    "    \n",
    "#list(y_test)\n",
    "visualize_prediction(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later you can load it using keras.models.load_model()\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(cache_dir, model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12500 samples, validate on 12500 samples\n",
      "Epoch 1/3\n",
      "12500/12500 [==============================] - 242s 19ms/step - loss: 0.1420 - acc: 0.9497 - val_loss: 0.2461 - val_acc: 0.9067\n",
      "Epoch 2/3\n",
      "12500/12500 [==============================] - 241s 19ms/step - loss: 0.1280 - acc: 0.9554 - val_loss: 0.2189 - val_acc: 0.9192\n",
      "Epoch 3/3\n",
      "12500/12500 [==============================] - 242s 19ms/step - loss: 0.1107 - acc: 0.9626 - val_loss: 0.3451 - val_acc: 0.9020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c1c7406d8>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 346s 23ms/step - loss: 0.1238 - acc: 0.9583 - val_loss: 0.2650 - val_acc: 0.9098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c1c7401d0>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, batch_size=32, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "msg=\"Such a loss of time \"\n",
    "def review_to_array(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    words_ids=[word2id.get(i, \" \") for i in text]\n",
    "    review_array=np.array(list(filter((\" \").__ne__, words_ids)))\n",
    "\n",
    "\n",
    "    test_seq = np.pad(review_array, (max_words-len(review_array), 0),\n",
    "                      'constant', constant_values=(0),)\n",
    "\n",
    "    \n",
    "    test_seq = test_seq.reshape(-1, 500)\n",
    "\n",
    "    return test_seq\n",
    "\n",
    "\n",
    "\n",
    "def get_predicion(msg):\n",
    "    msg  = review_to_array(msg)\n",
    "    pred = model.predict_classes(msg)\n",
    "    \n",
    "    return 'Positive' if pred[0]==1 else 'Negative'\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "pred = get_predicion(msg)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your movie review.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aab48aa08f44b783b290efb063cc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Text</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdf\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "You liked the movie.\n",
      "I hate it\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "You liked the movie.\n",
      "I Love it but \n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "You didn't like that movie.\n",
      "Amazing\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "You liked the movie.\n",
      "terrible\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "You liked the movie.\n",
      "Terrible e\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "You liked the movie.\n",
      "Terrible \n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "You liked the movie.\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import widgets \n",
    "from IPython.display import display\n",
    "\n",
    "print('Please enter your movie review.')\n",
    "text = widgets.Text()\n",
    "\n",
    "display(text)\n",
    "def handle_submit(sender):\n",
    "    review=(text.value)\n",
    "    try:\n",
    "        res=(get_predicion(review))\n",
    "        printed= 'You liked the movie.'if res == 'Positive' else 'You didn\\'t like that movie.'\n",
    "        print(printed)\n",
    "        \n",
    "    except:\n",
    "        print('Please enter a more descriptive review for the movie/series')\n",
    "    \n",
    "\n",
    "  \n",
    "text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Comparing RNNs and Traditional Methods\n",
    "\n",
    "How well does your RNN model perform compared to the BoW + Gradient-Boosted Decision Trees?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "...\n",
    "\n",
    "## Extensions\n",
    "\n",
    "There are several ways in which you can build upon this notebook. Each comes with its set of challenges, but can be a rewarding experience.\n",
    "\n",
    "- The first thing is to try and improve the accuracy of your model by experimenting with different architectures, layers and parameters. How good can you get without taking prohibitively long to train? How do you prevent overfitting?\n",
    "\n",
    "- Then, you may want to deploy your model as a mobile app or web service. What do you need to do in order to package your model for such deployment? How would you accept a new review, convert it into a form suitable for your model, and perform the actual prediction? (Note that the same environment you used during training may not be available.)\n",
    "\n",
    "- One simplification we made in this notebook is to limit the task to binary classification. The dataset actually includes a more fine-grained review rating that is indicated in each review's filename (which is of the form `<[id]_[rating].txt>` where `[id]` is a unique identifier and `[rating]` is on a scale of 1-10; note that neutral reviews > 4 or < 7 have been excluded). How would you modify the notebook to perform regression on the review ratings? In what situations is regression more useful than classification, and vice-versa?\n",
    "\n",
    "Whatever direction you take, make sure to share your results and learnings with your peers, through blogs, discussions and participating in online competitions. This is also a great way to become more visible to potential employers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
